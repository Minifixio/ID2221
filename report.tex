% Very simple template for lab reports. Most common packages are already included.
\documentclass[a4paper, 11pt]{article}
\usepackage[utf8]{inputenc} % Change according your file encoding
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{url}
\usepackage{listings}
\usepackage[justification=centering]{caption}
\usepackage{titlesec}
\usepackage{tikz}
\usepackage{hyperref} 
\usetikzlibrary{positioning}
\usepackage{courier}
\usepackage{amsmath}

\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }
 
% Listing settings
\lstset{
  basicstyle=\ttfamily\scriptsize,
  keywordstyle=\bfseries,
  commentstyle=\itshape,
  showstringspaces=false,
  language=erlang,
  columns=flexible,
  frame=single,
  breaklines=true
}

%opening
\title{ID2221 | Project report}
\author{Group HIF (Anna Kovács, Emile Le Gallic, Alex Orlandi)}
\date{\today{}}

\begin{document}

\maketitle

\section{Introduction}

Anna

\section{Code Analysis}

To analyze the dataset and perform the network analysis, we parsed the JSON file, taking each paper’s \texttt{id} as a unique node identifier and each \texttt{references} field as an adjacency list representing edges from one paper to each paper it cites. By using the \texttt{id} as the node ID, we were able to build a structured graph representation of the data, allowing each referenced article to be included as a connected node if it exists in the dataset. This structure enabled us to work with a complete graph of the citation network, linking each paper to its referenced articles.

The approach used Apache Spark’s GraphX library, optimized for large-scale graph processing. We used Spark’s \texttt{DataFrame} API to read the JSON file and apply transformations to extract nodes (papers) and edges (citations). For the nodes, we collected each paper's ID and relevant metadata (e.g., \texttt{title}, \texttt{year}) to form the \texttt{VertexRDD}. Similarly, the \texttt{EdgeRDD} was generated by iterating over each paper’s references. Specifically, Spark transformations such as \texttt{map}, \texttt{filter}, and \texttt{reduceByKey} were used as follows:
\begin{itemize}
    \item \texttt{map}: to hash and transform each paper’s \texttt{id} into nodes and edges.
    \item \texttt{filter}: to retain only edges where both the source and target (cited) nodes exist in the dataset.
    \item \texttt{reduceByKey}: to aggregate edges, ensuring no redundant connections.
\end{itemize}
This workflow minimized memory usage by creating a dataset limited only to valid nodes and edges, ensuring efficient graph construction.

We also implemented a feature to compute the connected components of the graph using Spark GraphX's \texttt{connectedComponents} function, which assigns each connected subgraph a unique identifier. To filter for meaningful subgraphs and exclude isolated nodes, we added a minimum component size parameter, keeping only components with more than a specified number of nodes.

For exporting the graph data, we utilized Spark’s \texttt{mapPartitions} function within the \texttt{ExportGraph} module to generate output files in JSON and CSV formats. With \texttt{mapPartitions}, we process data one partition at a time instead of loading the entire dataset into memory, enabling us to write node and edge data incrementally. This approach is particularly effective for large datasets, where collecting and writing all data at once would be memory-intensive and could potentially cause heap memory errors. By using \texttt{mapPartitions}, we reduced memory usage, allowing each partition to be handled separately, thus making it feasible to process and export even large graphs efficiently.

\section{Results Showcase}

In this section, we present the results obtained from processing a 1.04 GB dataset of academic papers. The graph constructed from this dataset has the following properties:

\begin{itemize}
    \item \textbf{Number of papers in the dataset:} 272,965
    \item \textbf{Number of vertices in the whole graph:} 272,957
    \item \textbf{Number of edges in the whole graph:} 223,677
\end{itemize}

\subsection{Graph Filtering Based on Vertex Degree}

To reduce the graph size, we applied degree-based filtering on the vertices, where we only retained vertices with a minimum degree of 3 or 5. Below are the results for each filter level:

\begin{itemize}
    \item \textbf{Degree $\geq 3$}:
    \begin{itemize}
        \item Number of papers in the dataset: 272,965
        \item Number of vertices in the filtered graph: 53,607
        \item Number of edges in the filtered graph: 116,188
    \end{itemize}
    \item \textbf{Degree $\geq 5$}:
    \begin{itemize}
        \item Number of papers in the dataset: 272,965
        \item Number of vertices in the filtered graph: 22,461
        \item Number of edges in the filtered graph: 50,921
    \end{itemize}
\end{itemize}

\subsection{Connected Components Analysis}

When computing the connected components for the full graph, we encountered an issue: the largest connected component encompasses nearly all vertices, while the remaining components are very small. This makes it challenging to visualize the graph as separate, evenly-sized connected components.

The size distribution of the largest 10 connected components in the graph is as follows:

\begin{center}
\begin{tabular}{|c|c|}
\hline
\textbf{Cluster ID} & \textbf{Size} \\
\hline
-2147401561 & 124,862 \\
-1738317891 & 26 \\
-2104093093 & 26 \\
-2147259791 & 24 \\
-1878352363 & 22 \\
-2000070975 & 20 \\
-1878264462 & 19 \\
-2104110280 & 18 \\
-1003223759 & 18 \\
-1334968787 & 18 \\
\hline
\end{tabular}
\end{center}

\subsection{Explanation}

This behavior is likely due to how \texttt{Spark GraphX} calculates connected components, which are based on the underlying structure and density of connections within the graph. In many real-world graphs, especially social or citation networks, a “giant component” phenomenon occurs, where a single large connected component spans the majority of nodes, while other components are comparatively small. 

In our case, the majority of nodes are densely connected, forming one large component, while only a few nodes are isolated or form small clusters. This is common in citation networks, where certain papers are widely cited, creating dense interconnections that are reflected in Spark's connected components calculation. 

This structural density, combined with \texttt{GraphX}'s distributed processing approach, leads to an uneven distribution in connected component sizes, limiting control over maximum component size in the final output.



\section{Visualization}

To visualize the graph data, we implemented flexible export functionalities, allowing export in JSON and CSV formats. The \texttt{exportToJSON} function outputs a complete JSON file with nodes and edges suitable for web-based visualization frameworks, while the \texttt{exportToCSV} function enables users to visualize on platforms such as \href{https://cosmograph.app/}{Cosmograph}. The CSV export includes an \texttt{edges.csv} file with pairs of source and target nodes, and a \texttt{metadata.csv} file with additional node attributes such as title, color, and size.

To handle larger datasets efficiently, we used Spark’s \texttt{mapPartitions} function, which processes data partition-by-partition instead of loading it entirely into memory. This approach optimized memory usage and reduced runtime, enabling us to process larger datasets without performance bottlenecks.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{graph_labels.png} % Adjust file path if in a different folder
    \caption{Graph with vertices of degree $\geq 5$ (View 1)}
    \label{fig:graph_view1}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{graph_no_labels.png} % Adjust file path if in a different folder
    \caption{Graph with vertices of degree $\geq 5$ (View 2)}
    \label{fig:graph_view2}
\end{figure}


\section{Problems Encountered}

During processing, the dataset’s size presented computational challenges. The JSON file needed to be split into smaller files to fit within typical memory constraints. The file was stored in Hadoop Distributed File System (HDFS) to facilitate parallel access in Spark. HDFS proved advantageous, allowing us to split data for efficient, distributed processing and retry failed tasks.

One significant issue was the memory heap size when running Spark GraphX on a local machine. To prevent memory overflow errors, we increased the memory heap allocation by setting \texttt{SBT\_OPTS} to \texttt{-Xmx8g -XX:+UseG1GC}.

\section{Conclusion and Possible Improvements}

In this project, we successfully implemented a Spark GraphX pipeline to parse and analyze an academic citation dataset, handling scalability challenges associated with large datasets. We could explore connected components and export multiple graph formats, making it easy to integrate with external visualization tools.

Future improvements could include:
\begin{itemize}
    \item \textbf{Scaling Up:} Setting up a larger processing platform with greater computing power, such as a remote server cluster, could handle larger datasets and reduce processing time.
    \item \textbf{Graph Analysis:} With more computational resources, we could explore shortest path algorithms, such as finding the shortest path between two papers or identifying paths that connect specific KTH researchers' papers.
\end{itemize}

Overall, Spark GraphX proved to be a robust tool for network analysis on large-scale datasets, facilitating efficient and scalable graph computations. This project demonstrates the power of distributed graph processing, with Spark handling data transformation and GraphX performing complex graph operations.


\end{document}
